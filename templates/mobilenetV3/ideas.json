[
    {
        "Name": "universal_inverted_bottleneck",
        "Title": "Implementing and Evaluating the Universal Inverted Bottleneck (UIB) Block",
        "Experiment": "Implement the Universal Inverted Bottleneck (UIB) block. The UIB block extends the MobileNet Inverted Bottleneck (IB) block by adding two optional depthwise convolutions: one before the expansion layer and one between the expansion and projection layers. This flexible structure allows the UIB to instantiate four variants: 1) MobileNet Inverted Bottleneck, 2) ConvNext-Like, 3) ExtraDW (a novel variant with both optional depthwise convolutions), and 4) FFN (Feed-Forward Network). Construct several MobileNetV4-style models of varying sizes (e.g., Small, Medium, Large) using the UIB blocks. Evaluate the resulting models on ImageNet classification, measuring both accuracy and latency across a range of mobile hardware (CPUs, GPUs). Compare the performance and efficiency of these UIB-based models against baseline MobileNetV3 and other state-of-the-art efficient models.",
        "Interestingness": 9,
        "Feasibility": 7,
        "Novelty": 8
    },
    {
        "Name": "normalization_techniques_comparison",
        "Title": "Comparison of Different Normalization Techniques in Small Convolution Networks",
        "Experiment": "Modify the ConvNormActivation and InvertedResidual classes to support BatchNorm, LayerNorm, InstanceNorm, and GroupNorm. Train and evaluate models with each normalization technique on CIFAR-10. Compare the results based on accuracy, training loss, validation loss, training stability, and convergence speed. Metrics will be logged and compared across multiple runs to ensure statistical significance.",
        "Interestingness": 8,
        "Feasibility": 8,
        "Novelty": 7
    }
]